{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab139d-f343-40c9-9509-6cbc89093a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b91c84-5869-4414-8536-2f4e4e7c10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2373d50-b953-432d-b72f-5afaee44e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test mnist 121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e15c0f-d312-4acf-8ee3-2a4becf88365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MNIST Autoencoder file\n",
    "import MNIST_121  \n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define arguments\n",
    "args = argparse.Namespace(\n",
    "    data_path=\"./data\",\n",
    "    batch_size=8,\n",
    "    latent_dim=128,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0b5b8-8e7b-42a1-8c82-a77737c4eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset and Check It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710c0e86-7eef-4f8e-a0a0-7ba4afb6fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_loader, val_loader, test_loader = MNIST_121.load_data(args)\n",
    "\n",
    "# Retrieve a batch of images\n",
    "images, _ = next(iter(train_loader))\n",
    "\n",
    "# Visualize some images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    img = images[i].squeeze()\n",
    "    img = img.numpy()\n",
    "    axes[i].imshow(img, cmap=\"gray\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b56fb-3c14-466d-abc7-d508ea5d2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the Autoencoder Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822be85a-7dc4-435a-acc2-95d9285294bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MNIST_121.Autoencoder(args.latent_dim).to(args.device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231c79c-d687-4344-83b3-432beb2a9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Single Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb996c-53ef-4718-bd7b-53b9fabf4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "images, _ = next(iter(train_loader))\n",
    "images = images.to(args.device)\n",
    "reconstructed = model(images)\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {reconstructed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95db66-1ac3-4629-b66c-7801b2ff14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train for One Epoch to Check Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8b4e0-3c6c-41f4-9e5e-1ba5de87662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_121.train_autoencoder(model, train_loader, val_loader, args, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff363fb-0c53-4ba9-862c-5f510491a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c18891-4b01-40fe-89fd-496a9b93c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test batch\n",
    "images, _ = next(iter(test_loader))\n",
    "images = images.to(args.device)\n",
    "\n",
    "# Generate reconstructed images\n",
    "reconstructed = model(images).detach().cpu()\n",
    "\n",
    "# Plot original vs reconstructed images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(images[i].squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(reconstructed[i].squeeze().numpy(), cmap=\"gray\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_title(\"Original\", fontsize=12)\n",
    "axes[1, 0].set_title(\"Reconstructed\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e60b0-5cbb-4689-b88b-ad41f43049e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test mnist 122\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925290c-7b17-4b7a-b8d5-cf155fb9199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MNIST_122 module\n",
    "import MNIST_122  \n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define arguments\n",
    "args = argparse.Namespace(\n",
    "    data_path=\"./data\",\n",
    "    batch_size=8,\n",
    "    latent_dim=128,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff669b9-c38d-473d-8bc1-4a0798225d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset and Check It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6c66e-f925-4ebc-ae79-0390d70968bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_loader, val_loader, test_loader = MNIST_122.load_data(args)\n",
    "\n",
    "# Retrieve a batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Visualize images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    img = images[i].squeeze()\n",
    "    img = img.numpy()\n",
    "    axes[i].imshow(img, cmap=\"gray\")\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(labels[i].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef28c5-7f4e-4dc0-86ff-d5cd4debfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify Encoder + Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f378ac-0981-4525-9337-c6bcf12b1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MNIST_122.Classifier(args.latent_dim).to(args.device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de05b6-955b-4ecb-b8dc-97478b8349d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d798bea-729d-4cbf-be31-08fbd8ef911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "images, _ = next(iter(train_loader))\n",
    "images = images.to(args.device)\n",
    "outputs = model(images)\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")  # Should be [batch_size, 10] for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92596e32-e7a1-46f4-aff9-3deaeeb417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6328325-d291-4ac2-8690-03003d51ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_122.train_classifier(model, train_loader, val_loader, args, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99304eba-6808-4691-97c1-2ea4e0df6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2f7bc-c098-48e8-8fbf-f4295fc5c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e4bb6-64d7-4f18-ba41-3a61c6e839bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test cifar10 121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441be6c-1533-4649-98bc-623ff9ddbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CIFAR10_121 module\n",
    "import CIFAR10_121  \n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define arguments\n",
    "args = argparse.Namespace(\n",
    "    data_path=\"./data\",\n",
    "    batch_size=8,\n",
    "    latent_dim=128,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e086e-123a-4eb6-85dd-b0e78a61d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset & Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403de97-4b2f-41fd-b06e-96c8d5a628a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_loader, val_loader, test_loader = CIFAR10_121.load_data(args)\n",
    "\n",
    "# Retrieve a batch of images\n",
    "images, _ = next(iter(train_loader))\n",
    "\n",
    "# Visualize images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    img = images[i].permute(1, 2, 0).numpy()  # Convert from PyTorch format to image format\n",
    "    img = (img * 0.5) + 0.5  # Reverse normalization\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf93617-7dce-4b13-bb2e-5a14df20e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e44529-36ee-4b26-beb9-ac210876f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CIFAR10_121.Autoencoder(args.latent_dim).to(args.device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adb202-bb05-4f7a-82f8-e4ec85500171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b615121-de0d-477a-b73f-bd752d0cfa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "images, _ = next(iter(train_loader))\n",
    "images = images.to(args.device)\n",
    "outputs = model(images)\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")  # Should match the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc796fd-baef-4a42-902c-4c6e84d2e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Train for One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46980cd9-0ce9-4cf9-9b7d-41b64fc05c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_121.train_autoencoder(model, train_loader, val_loader, args, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c19b1-6e71-4662-bacf-84754b73ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97aa95-8c2a-4069-ae34-7575621559e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(model, test_loader):\n",
    "    model.eval()\n",
    "    images, _ = next(iter(test_loader))\n",
    "    images = images.to(args.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructions = model(images)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "    \n",
    "    for i in range(8):\n",
    "        # Original image\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img * 0.5) + 0.5  # Reverse normalization\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        # Reconstructed image\n",
    "        recon = reconstructions[i].cpu().permute(1, 2, 0).numpy()\n",
    "        recon = (recon * 0.5) + 0.5\n",
    "        axes[1, i].imshow(recon)\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "    axes[0, 0].set_title(\"Original\")\n",
    "    axes[1, 0].set_title(\"Reconstructed\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstruction(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32073bc5-11ee-4530-8ef9-3bb1acbf85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test cifar10 122\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bdcdd-93f0-4dc0-bef1-3367a9bec3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CIFAR10_122 module\n",
    "import CIFAR10_122  \n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define arguments\n",
    "args = argparse.Namespace(\n",
    "    data_path=\"./data\",\n",
    "    batch_size=8,\n",
    "    latent_dim=128,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857b652-bb19-4a27-b193-fc020934a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset & Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10259a86-1fca-423b-b5c1-40d85c7745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_loader, val_loader, test_loader = CIFAR10_122.load_data(args)\n",
    "\n",
    "# Retrieve a batch of images\n",
    "images, _ = next(iter(train_loader))\n",
    "\n",
    "# Visualize images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    img = images[i].permute(1, 2, 0).numpy()  # Convert from PyTorch format to image format\n",
    "    img = (img * 0.5) + 0.5  # Reverse normalization\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c03f9a-27a1-4381-a7b4-5b05c1528bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify Encoder + Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5593dd-cfa4-4ca9-a0a7-5a6a16f7c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CIFAR10_122.Classifier(args.latent_dim).to(args.device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d0f32-f693-4fcd-b629-abfe355c45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d28f5-2468-4f4d-954d-9b7bb692a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "images, _ = next(iter(train_loader))\n",
    "images = images.to(args.device)\n",
    "outputs = model(images)\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")  # Should match (batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466983e-69ef-469e-94fe-eb9e8c7a5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train for One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a198dfd-f6a7-4db3-aa20-715ffab47ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_122.train_classifier(model, train_loader, val_loader, args, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670db9e2-0c54-407f-86c9-47fff2111bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b8549-c92d-40dd-9745-602bc3b260fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n",
    "\n",
    "evaluate_classifier(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761f643-989c-4335-8201-dd07e753d77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
